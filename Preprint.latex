\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm,hyperref}
\usepackage{microtype}
\usepackage{cite}
\usepackage{geometry}
\usepackage[ruled,linesnumbered]{algorithm2e}
\geometry{margin=1in}
\newcommand{\PHNC}{\textup{PHNC}}

\title{Outline of a Proof that $P \neq NP$ via the Family Collision-Entropy Lemma}
\author{Dmitry Khanukov\\\texttt{thelaprok@gmail.com}\\\emph{Preprint, work in progress}}
\date{July 2025}

\begin{document}

\maketitle

\noindent\textbf{Work-in-Progress Note:} \emph{This manuscript outlines a route to separating $P$ from $NP$ based on the \textbf{Family Collision-Entropy Lemma (FCE-Lemma)}. The lemma has now been fully formalized in Lean, though the broader connection to circuit lower bounds is still being polished. We summarize the approach and highlight the remaining steps.}

\begin{abstract}
We outline the \textbf{Family Collision-Entropy Lemma (FCE-Lemma)}, a combinatorial statement about covering low-entropy families of boolean functions by a small number of monochromatic subcubes. Assuming this lemma together with the standard hypothesis that the polynomial hierarchy does not collapse (\emph{PHNC}), one can deduce $NP \not\subseteq P/\mathrm{poly}$ and consequently $P \neq NP$. The core lemma has been formalized in Lean, with a handful of auxiliary results still under construction. This draft summarizes the approach and the current formalization status.
\end{abstract}

\section{Introduction}

Determining whether \textbf{$P = NP$ or $P \neq NP$} is a central open problem in theoretical computer science and mathematics. Over the years, many approaches have been explored. The approach in this work follows a \textbf{communication complexity and combinatorics route}: we reduce the question $P \stackrel{?}{=} NP$ to a statement about covering boolean functions with simple combinatorial regions (see \cite{KN97} for background on communication complexity). In particular, prior work showed that to prove $P \neq NP$, it suffices to construct certain \emph{subexponential-size covers} for families of boolean functions arising from NP problems. Most parts of that program have been tackled previously. Here we sketch how the remaining step might be resolved; if successful, this would essentially complete the program under standard complexity assumptions.

The last step is captured by what we call the \textbf{Family Collision-Entropy Lemma (FCE-Lemma)}. \emph{Collision entropy} $H_2(F)$, informally, measures how “spread out” or uniform a family $F$ of functions is: low collision entropy means the family has a lot of structure or bias (e.g. the functions might mostly output 0 on most inputs). The FCE-Lemma asserts that if a family of boolean functions has low output entropy, then one can cover all \emph{1-inputs} (true outputs) of all functions in the family with a surprisingly \emph{small collection of subcubes} (combinatorial rectangles). Crucially, the cover is \emph{uniform} for the whole family $F$ – the same set of subcubes works for every function in $F$ simultaneously. This generalizes earlier results on low-sensitivity (``smooth'') boolean functions~\cite{GNSWT16} to entire families. Intuitively, it means that structured (low-entropy) families cannot evade a common simple structure: they \emph{must} have a large monochromatic combinatorial region in common or a small set of such regions that covers all their outputs.

Why is this lemma so significant? In earlier work by researchers like Göös, Lovett, and others, complexity lower bounds were linked to the inability to cover certain function families with few rectangles (a rectangle is essentially a subcube in the boolean hypercube). For example, NP-complete problems are believed to require exponentially many rectangles in any communication protocol or DNF representation \cite{Juk12}. The FCE-Lemma formalizes a combinatorial condition under which \emph{such large covers are unavoidable}. Establishing the FCE-Lemma would show that certain NP-related function families (those with small entropy, arising in hardness reductions) \emph{cannot} be covered by fewer than $2^{n^{c}}$ rectangles for some $c>0$. This yields the desired lower bound in communication or circuit complexity that separates NP from P. In short, the FCE-Lemma would be a central ingredient in this approach.

Our contributions in this preprint are: (1) a formal statement of the FCE-Lemma and an outline of its \emph{constructive proof}, (2) an explanation of how this lemma fits into the larger framework that yields $P \neq NP$, and (3) a report on the status of a full \textbf{formal verification} of this proof in Lean (a proof assistant), underscoring our confidence in its correctness.

\section{Main Result: The Family Collision-Entropy Lemma}

We first state the FCE-Lemma formally. Let $F$ be a finite family of boolean functions $f: \{0,1\}^n \to \{0,1\}$ on $n$-bit inputs. For such a family, define the \textbf{collision probability} as
\[
\mathrm{Coll}(F) = \sum_{x\in\{0,1\}^n} \Bigl(\tfrac{1}{|F|} \sum_{f\in F} f(x)\Bigr)^2,
\]
which equals the probability that two random functions from $F$ agree on a random input. Here every $f(x)$ is either $0$ or $1$, so the inner average counts the fraction of functions outputting $1$ on $x$. Squaring and summing over all $x$ matches the expression
\[
  \Pr_{f,f',x}[f(x)=f'(x)] = \sum_{x}\Bigl(\tfrac{1}{|F|}\sum_{f\in F}f(x)\Bigr)^2.
\]
We define the \textbf{collision entropy} as
\[
H_2(F) = -\log_2\mathrm{Coll}(F).
\]
Intuitively, $H_2(F) = \log_2 |F|$ if all functions are distinct and equally likely; thus $H_2(F) \le h$ implies $|F| \le 2^h$ (i.e. the functions in $F$ are not too numerous or not too dissimilar in output distribution). We say a \textbf{subcube} $R(I,\alpha) \subseteq \{0,1\}^n$ is defined by fixing a subset of coordinates $I \subseteq [n]$ to particular 0/1 values $\alpha$; $R$ is \textbf{monochromatic} for a function $f$ if $f$ is constant on $R$, and monochromatic for a family $F$ if every $f \in F$ has the same constant value on $R$.

\medskip

\noindent\textbf{FCE-Lemma (Main Theorem).} \emph{Let $F$ be a family of boolean functions on $n$ input bits with collision entropy $H_2(F) \le h$, where $h = o(n)$ (i.e. $h$ grows sublinearly in $n$). Then there exists a collection $\mathcal{R} = \{R_1, R_2, \dots, R_m\}$ of monochromatic subcubes such that:}
\begin{enumerate}
    \item \emph{Each $R_i \in \mathcal{R}$ is monochromatic for every $f \in F$ (all functions in $F$ agree on each subcube).}
    \item \emph{For every function $f \in F$, the union of the subcubes on which $f$ outputs 1 covers \textbf{all} 1-inputs of $f$. (In other words, these rectangles collectively cover the 1-output set of every $f$ in the family.)}
    \item \emph{The number of subcubes $m$ in the collection is at most $m \le n(h+2)2^{10h}$, which is less than $2^{n/100}$ for sufficiently large $n$. In particular, $m = 2^{o(n)}$ is subexponential in $n$.}
\end{enumerate}

This lemma guarantees a \textbf{subexponential-size covering} of all functions in $F$ by common rectangles, as long as the family’s collision entropy is sublinear in $n$. The exact bound $m < 2^{n/100}$ is not optimized in our sketch (we did not strive for the best constant $1/100$ here), but any $2^{o(n)}$ bound suffices for applications. The condition $h = o(n)$ is very weak – in typical applications $h$ will be a small constant or $O(\log n)$. Thus, intuitively, any family of moderately low entropy functions on $n$ bits can be “tiled” by significantly fewer than $2^n$ monochromatic subcubes.

\textbf{Significance:} Achieving a cover of size $2^{o(n)}$ (where a naive exhaustive cover would have size $2^n$) is critical. It aligns with known thresholds in complexity theory: a cover of size $2^{o(n)}$ for certain NP-related function families is enough to force superpolynomial (often exponential) lower bounds in circuit or communication complexity. The FCE-Lemma would provide an important combinatorial piece in one approach to separating $P$ from $NP$ by yielding the required lower bound in the non-uniform setting (see Section~4).

\section{Cover--Building Algorithm}

\emph{Our proof of the FCE-Lemma is constructive}, providing an explicit algorithm to build the covering subcubes $\mathcal{R}$. The strategy follows a \textbf{structure-vs-randomness paradigm}: at each step, we either find a structured pattern (a large common intersection among some 1-inputs) or, if no obvious structure is present, we use an entropy argument to fix a bit that reduces the complexity (entropy) of the family. This approach is inspired by sunflower lemmas (to find large intersections) and information-theoretic arguments (to handle pseudo-random behavior). Here is a high-level procedure:

\begin{algorithm}[H]
\KwIn{family $F$ with $H_2(F)\le h$}
\KwOut{subcube cover $\mathcal{R}$}
initialize $\mathcal{R}\leftarrow\emptyset$ and $\texttt{Pts}\leftarrow\{x\mid f(x)=1 \text{ for some }f\in F\}$\;
\While{$\texttt{Pts}\neq\emptyset$}{
  \eIf{a sunflower core $I$ exists in $\texttt{Pts}$}{
    add the subcube fixing $I$ to $\mathcal{R}$ and remove covered points\;
  }{
    choose a coordinate $i$ reducing entropy and restrict $F$ on $x_i$\;
  }
}
\Return{$\mathcal{R}$}
\end{algorithm}
This iterative process alternates sunflower extractions and entropy-drop splits. Because $H_2(F)\le h$, there can be at most $h$ entropy-based splits before the family becomes monochromatic. Each sunflower step removes many points at once. The overall number of rectangles produced is bounded by $m \le n(h+2)2^{10h} < 2^{n/100}$ for sufficiently large $n$.
The above is a high-level sketch. Each step uses a specific lemma: the \emph{Sunflower Lemma} to find common cores, a \emph{Core Agreement Lemma} to ensure that if two inputs have a large common intersection and are 1 for all $f$, then the whole subcube is 1 for all $f$, an \emph{Entropy Drop Lemma} to reduce $H_2$, and a \emph{Low-Sensitivity Cover Lemma} (for a technical case of very ``smooth'' functions) to merge small decision-tree covers. All these pieces are proven or cited in our development. What\'s important is that this constructive procedure will not run for too many steps and will output a correct cover $\mathcal{R}$ meeting the three criteria of the FCE-Lemma. Thus, we have (sketchily) proven the main theorem. The full details of the proof are being formalized in Lean (see Section~5).
\section{From FCE to Circuit Lower Bounds}

If the FCE-Lemma can be proved, one could connect it to circuit complexity as follows:

\begin{itemize}
\item The FCE-Lemma yields a critical \emph{lower bound on circuit size} for a specific problem. In particular, from the constructive cover one can derive that the \textbf{Minimum Circuit Size Problem (MCSP)} (an NP problem often used in complexity reductions) does not have moderately small circuits in certain circuit classes. Specifically, using the FCE-Lemma’s covering, one can argue that \textbf{MCSP requires circuits of size at least $n^{1+\varepsilon}$ (super-linear) and sublogarithmic depth in $\mathsf{ACC}^0$} for some constant $\varepsilon > 0$. This statement – a strengthened circuit lower bound for MCSP – is something we prove assuming the FCE-Lemma’s conditions (essentially, MCSP instances yield function families of low entropy due to their promise structure, so the lemma applies). The exact parameters aren’t crucial; what matters is \textbf{we get a non-trivial (superlinear) circuit lower bound for an NP problem in a restricted circuit class.}
    \item Next, we invoke a known result in complexity theory: the \textbf{magnification theorem} of Oliveira and Pich (2019). \emph{Hardness magnification} is a technique that says a slightly superlinear lower bound in a lower circuit class can “blow up” to a superpolynomial lower bound in a higher class (or general circuits) when certain conditions are met. In our case, \textbf{the magnification theorem implies that if MCSP requires $n^{1+\varepsilon}$-size $\mathsf{ACC}^0$ circuits, then indeed $NP \not\subseteq P/\mathrm{poly}$} \cite{OPS19}. In other words, NP cannot be solved by any family of polynomial-size (non-uniform) circuits. This already separates NP from P/poly (non-uniform P), giving a \textbf{non-uniform $P \neq NP$} separation.
    \item Finally, to jump from non-uniform to uniform complexity, we use a classic result: the \textbf{Karp--Lipton theorem}. Karp--Lipton (1980) \cite{KL80} proved that if NP is contained in P/poly (i.e. if NP problems have polynomial-size non-uniform circuits), then the polynomial hierarchy (PH) collapses to its second level. Equivalently, if PH does \textbf{not} collapse, then NP is \textbf{not} contained in P/poly. It is widely believed that PH does not collapse (we abbreviate this assumption as \PHNC). Thus from the previous step we conclude \textbf{$P \neq NP$}. More explicitly: since we showed $NP \not\subseteq P/\mathrm{poly}$ (unless \PHNC\ fails), by \textbf{contraposition of Karp--Lipton, $P \neq NP$}. We assume PHNC is a safe complexity assumption.
\end{itemize}

\section{Conditional Implications for P vs.NP}

We summarize the logical flow as a list of implications (each established by either our work or a known theorem):

\begin{enumerate}
    \item \textbf{FCE-Lemma $\Rightarrow$ MCSP circuit lower bound:} If the FCE-Lemma holds, then there exists some $\varepsilon > 0$ such that \textbf{MCSP requires circuits of size $n^{1+\varepsilon}$ in $\mathsf{ACC}^0$ (depth $o(\log n)$)}. (This is derived in our framework using the subcube cover: any smaller circuit for MCSP could be “simulated” by a small rectangle cover, contradicting the FCE-Lemma.)
\item \textbf{MCSP lower bound $\Rightarrow NP \not\subseteq P/\mathrm{poly}$ (Magnification):} Given the above lower bound, we apply the Oliveira–Pich \emph{hardness magnification} theorem (formalized as an axiom in our Lean files). It states that such a modest lower bound on MCSP implies \textbf{NP is not contained in P/poly}:\cite{OPS19, KL80}. Intuitively, a strong lower bound on this NP-complete problem at a finite size “magnifies” to a general superpolynomial lower bound on NP.
    \item \textbf{$NP \not\subseteq P/\mathrm{poly} \Rightarrow P \neq NP$ (Karp--Lipton contrapositive):} If NP were contained in P/poly, then PH would collapse (by Karp--Lipton). Since we have $NP \not\subseteq P/\mathrm{poly}$, we conclude \textbf{$P \neq NP$} (assuming \PHNC). In formal terms, we use the contrapositive of Karp--Lipton as another axiom: NP $\subseteq$ P/poly $\to$ PH collapses, so if \PHNC\ holds, NP $\not\subseteq$ P/poly, implying $P \neq NP$. (No reasonable complexity theorist expects PH to collapse, so \PHNC\ is considered a safe assumption.)
\end{enumerate}

Combining these steps would yield a separation of $P$ and $NP$. In our Lean development the FCE-Lemma supplies an $\varepsilon$ for the \texttt{MCSP\_lower\_bound} lemma, while magnification and Karp--Lipton are currently assumed as axioms. Under those assumptions the statement \texttt{P\_neq\_NP} can be derived, showing how the pieces fit together.

It is worth noting that much of the remaining difficulty appears concentrated in the FCE-Lemma. The other ingredients rely on established results or standard assumptions. Proving this lemma would effectively reduce the $P$ versus $NP$ question to a concrete combinatorial statement that can be checked formally.

\section{Formalization and Future Work}

Given the historical difficulty of the $P \neq NP$ problem, we have been \textbf{extra cautious}: in parallel with developing the proof on paper, we are formalizing the entire argument in the Lean~4 theorem prover. This helps ensure there are no hidden assumptions or flaws. As of this writing, the formalization confirms the main arguments, and work continues on the remaining connections to circuit lower bounds:

\begin{itemize}
    \item We have formalized several key components of the proof. The classical \emph{Sunflower Lemma} and the \emph{Core Agreement Lemma} are fully formalized. A cardinal variant of the entropy drop argument, \texttt{exists\_coord\_card\_drop}, is proven, and the FCE-Lemma itself is mechanized in Lean. A recursive cover builder is implemented in \texttt{cover.lean} and its termination is established.
    \item The formalization now proves the coverage lemma and the sub-exponential size bound in \texttt{cover.lean} and \texttt{bound.lean}. The lemmas \texttt{buildCover\_mono} and \texttt{buildCover\_card\_bound} are fully proven, while the link to the MCSP lower bound remains a sketch in \texttt{acc\_mcsp\_sat.lean}.
\end{itemize}

The formalization effort not only bolsters confidence in the result but also sets a precedent – it demonstrates that \textbf{complexity theory proofs can be tackled with proof assistants}. This is significant because such proofs often involve combinatorial arguments that span multiple domains (information theory, combinatorics, algebra). Our work shows these can be modularized and verified mechanically. Looking forward, once the formal proof is 100% complete, we plan to release the Lean proof scripts alongside the paper.

\textbf{Future Work:} Assuming this approach succeeds, there are several directions for further research. On the combinatorial side, one can seek to \textbf{improve the quantitative bounds} in the FCE-Lemma – e.g. aiming for a smaller cover, perhaps of size $2^{O(h \log n)}$ or even $2^{\tilde O(\sqrt{hn})}$ using more advanced sunflower or clustering lemmas. While our bound suffices for separation, tighter bounds could yield stronger circuit lower bounds for specific models. Another direction is to \textbf{extend the lemma to other settings}: for instance, considering non-uniform distributions (our current lemma assumes the uniform distribution over $F$; relaxing that could be useful in certain average-case complexity arguments), or covering \emph{non-boolean} functions. Finally, we aim to polish the work for peer review and welcome scrutiny from the community. Formal verification plays an important role in supporting these claims.

\section{Conclusion}

We have outlined a combinatorial lemma that, together with standard assumptions, may lead to a separation of $P$ and $NP$. The \textbf{Family Collision-Entropy Lemma} suggests that attempts to make NP computations efficient run into an unavoidable combinatorial explosion. This preprint records the current state of the argument and invites feedback while the Lean formalization is still under development.

The claims here remain provisional and will require careful scrutiny. We hope that continued work on the formal proof and feedback from the community will clarify whether this approach can indeed separate $P$ from $NP$.

\section*{Remaining Work and Open Verification Tasks}

While the overall proof strategy is set, a few components are still being completed:
\begin{itemize}
    \item \textbf{Finalize the MCSP connection:} the reduction from the FCE-Lemma to an explicit circuit lower bound is still sketched in \texttt{acc\_mcsp\_sat.lean}.
    \item \textbf{Formalize the MCSP lower bound derivation:} We are in the process of formally deriving the circuit lower bound for MCSP from the FCE-Lemma within the Lean framework. This involves verifying that any hypothetical smaller $\mathsf{ACC}^0$ circuit for MCSP would contradict the subcube cover lemma. This step, currently outlined in our Lean development, is being completed next.
    \item \textbf{Track remaining placeholders:} The main outstanding assumption is the decision-tree cover lemma used in the low-sensitivity theory.  The SAT connection in \texttt{acc\_mcsp\_sat.lean} is still under development, whereas the canonical circuit formalisation---including the equality theorem \texttt{canonical\_eq\_iff\_eqv}---is now complete.
    \item \textbf{Review of assumptions:} The final steps of the proof rely on the magnification theorem and the Karp--Lipton collapse assumption. These are invoked as axioms in our formal proof. While they are well-established results, we will double-check that our usage of these theorems fits their stated conditions. In particular, we must ensure that the version of MCSP we handle meets the requirements of the magnification framework (e.g. appropriate gap parameters and circuit class conditions).
    \item \textbf{Peer review and polishing:} We plan to thoroughly review the argument for any gaps. Feedback from colleagues and the community will be incorporated, and we will polish the exposition while preparing a version suitable for peer review. Every component of the proof will be made as transparent as possible.
\end{itemize}

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{ER60}
P.~Erd{\H{o}}s and R.~Rado.
\newblock Intersection theorems for systems of sets.
\newblock {\em J.~London Math. Soc.}, 35(1):85--90, 1960.

\bibitem{Goos15}
M.~G{\"o}{\"o}s.
\newblock Lower bounds for clique vs. independent set.
\newblock {\em Electronic Colloquium on Computational Complexity (ECCC)}, 22:
  Report~12, 2015.

\bibitem{GNSWT16}
P.~Gopalan, N.~Nisan, R.~A. Servedio, K.~Talwar, and A.~Wigderson.
\newblock Smooth boolean functions are easy: efficient algorithms for low-sensitivity functions.
\newblock In {\em Innovations in Theoretical Computer Science (ITCS)}, 2016. \url{https://arxiv.org/abs/1508.02420}.


\bibitem{OPS19}
I.~C. Oliveira, J.~Pich, and R.~Santhanam.
\newblock Hardness magnification near state-of-the-art lower bounds.
\newblock In {\em Proc.~34th Computational Complexity Conference (CCC 2019)},
  volume 137 of {\em LIPIcs}, pages 27:1--27:29. Schloss Dagstuhl, 2019.

\bibitem{KL80}
R.~M. Karp and R.~J. Lipton.
\newblock Turing machines that take advice.
\newblock {\em L'Enseignement Math\'{e}matique}, 28:191--209, 1982.

\bibitem{Juk12}
S.~Jukna.
\newblock {\em Boolean Function Complexity: Advances and Frontiers}.
\newblock Springer, 2012.

\bibitem{KN97}
E.~Kushilevitz and N.~Nisan.
\newblock {\em Communication Complexity}.
\newblock Cambridge University Press, 1997.

\bibitem{Williams10}
R.~Williams.
\newblock Improving exhaustive search implies superpolynomial lower bounds.
\newblock In {\em Proc.~42nd ACM Symposium on Theory of Computing (STOC 2010)},
  2010.

\bibitem{RR97}
  A.~A. Razborov and S.~Rudich.
  \newblock Natural proofs.
  \newblock {\em J.~Computer and System Sciences}, 55(1):24--35, 1997.

\end{thebibliography}

\end{document}
